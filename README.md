# Digital Transformation Project for Cafe Nogwon – Data Collection Automation (Crawling)


## Abstract
This project addresses the lack of accessible sales data and reliance on experience-based decision-making despite years of operation.  
The overall goal was to enable data-driven management by automating data collection, building a unified database, and supporting analysis and operational optimization.

This repository focuses specifically on the **data collection (crawling) component** that I was responsible for, automating the extraction of sales data from multiple external platforms to establish a reliable foundation for downstream analysis, visualization, and scheduling optimization.



## Problem
- Sales data was distributed across multiple platforms with different access methods and formats.
- Manual collection was inefficient and error-prone.
- There was no unified, repeatable process for acquiring data.


## Key Findings
- Reliable data collection is a prerequisite for effective analysis.
- Isolating platforms reduces the impact of UI changes.
- Simple automation logic is more robust than complex pipelines.


## Approach
- Implemented **platform-specific crawlers** using Selenium where APIs were unavailable.
- Separated **crawling** and **preprocessing** to improve maintainability.
- Externalized credentials and paths via environment variables.
- Avoided unnecessary abstractions to keep the system transparent and debuggable.



## Project Structure
```plaintext
src/
├── main.py          # Execution entry point
├── config.py        # Environment-based configuration
├── crawlers/        # Platform-specific crawling logic
├── preprocess/      # Platform-specific data preprocessing
└── utils/           # Shared utilities (browser, files, dates, logging)
```

## Code
- `main.py`: Entry point that orchestrates crawling and preprocessing for one or multiple platforms.
- `crawlers/`: Contains platform-specific Selenium crawlers. Each crawler is isolated to minimize the impact of UI or DOM changes.
- `preprocess/`: Handles transformation of raw collected data into structured, analysis-ready Excel files.
- `utils/`: Shared utilities for browser setup, file handling, date conversion, and logging.

## Tools and Libraries
- Python
- Selenium
- pandas, numpy
- BeautifulSoup
- openpyxl, xlrd

## Contribution
- Designed and implemented the crawling architecture.
- Developed automated data collection for multiple external platforms.
- Built preprocessing pipelines to produce structured Excel outputs.
